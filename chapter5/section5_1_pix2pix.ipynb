{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "section5_1-pix2pix.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iSKKkvnUXzc"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S72qcmh4ZRma"
      },
      "source": [
        "## データセットの取得"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETL3Y6nx9vOE"
      },
      "source": [
        "# facadesデータセットのダウンロード\n",
        "# cityscapes, night2day, edges2handbags, edges2shoes, facades, mapsも同様\n",
        "!wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
        "!mkdir -p ./datasets/facades\n",
        "!tar -zxvf facades.tar.gz -C ./datasets/\n",
        "!rm facades.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvU58taXNw-V"
      },
      "source": [
        "# 学習データの確認\n",
        "import matplotlib.pyplot as plt\n",
        "img = plt.imread('datasets/facades/train/1.jpg')\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luzItelLV1Hn"
      },
      "source": [
        "## ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD2zB4wPjPRv"
      },
      "source": [
        "!pip install torch>=0.4.1\n",
        "!pip install torchvision>=0.2.1\n",
        "!pip install dominate>=2.3.1\n",
        "!pip install visdom>=0.1.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R7WLgcLWkWl"
      },
      "source": [
        "## インポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrG3UxpSdCjo"
      },
      "source": [
        "import sys\n",
        "import argparse\n",
        "import os.path\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from PIL import Image\n",
        "from visdom import Visdom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHKXvNmKWoD1"
      },
      "source": [
        "# 関数定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqI3sUDpYdxY"
      },
      "source": [
        "## ペア画像のデータ生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gws59HzJYck2"
      },
      "source": [
        "# 条件画像と正解画像のペアデータセット生成クラス\n",
        "class AlignedDataset(Dataset):\n",
        "    IMG_EXTENSIONS = ['.png', 'jpg']\n",
        "    # configは全ての学習条件を格納する\n",
        "\n",
        "    # 画像データは'/path/to/data/train'および'/path/to/data/test'に\n",
        "    # {A,B}の形式で格納されているものとみなす\n",
        "\n",
        "    def __init__(self, config):\n",
        "        # データセットクラスの初期化\n",
        "        self.config = config\n",
        "\n",
        "        # データディレクトリの取得\n",
        "        dir = os.path.join(config.dataroot, config.phase)\n",
        "        # 画像データパスの取得\n",
        "        self.AB_paths = sorted(self.__make_dataset(dir))\n",
        "\n",
        "    @classmethod\n",
        "    def is_image_file(self, fname):\n",
        "        # 画像ファイルかどうかを返す\n",
        "        return any(fname.endswith(ext) for ext in self.IMG_EXTENSIONS)\n",
        "\n",
        "    @classmethod\n",
        "    def __make_dataset(self, dir):\n",
        "        # 画像データセットをメモリに格納\n",
        "        images = []\n",
        "        assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
        "\n",
        "        for root, _, fnames in sorted(os.walk(dir)):\n",
        "            for fname in fnames:\n",
        "                if self.is_image_file(fname):\n",
        "                    path = os.path.join(root, fname)\n",
        "                    images.append(path)\n",
        "        return images\n",
        "\n",
        "    def __transform(self, param):\n",
        "        list = []\n",
        "\n",
        "        load_size = self.config.load_size\n",
        "\n",
        "        # 入力画像を一度286x286にリサイズし、その後で256x256にランダムcropする\n",
        "        list.append(transforms.Resize([load_size, load_size], Image.BICUBIC))\n",
        "\n",
        "        (x, y) = param['crop_pos']\n",
        "        crop_size = self.config.crop_size\n",
        "        list.append(transforms.Lambda(lambda img: img.crop((x, y, x + crop_size, y + crop_size))))\n",
        "\n",
        "        # 1/2の確率で左右反転する\n",
        "        if param['flip']:\n",
        "            list.append(transforms.Lambda(lambda img: img.transpose(Image.FLIP_LEFT_RIGHT)))\n",
        "\n",
        "        # RGB画像をmean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)にNormalizeする\n",
        "        list += [transforms.ToTensor(),\n",
        "                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "\n",
        "        return transforms.Compose(list)\n",
        "\n",
        "    def __transform_param(self):\n",
        "        x_max = self.config.load_size - self.config.crop_size\n",
        "        x = random.randint(0, np.maximum(0, x_max))\n",
        "        y = random.randint(0, np.maximum(0, x_max))\n",
        "\n",
        "        flip = random.random() > 0.5\n",
        "\n",
        "        return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 学習用データ１つの生成\n",
        "        # A(テンソル) : 条件画像\n",
        "        # B(テンソル) : Aのペアとなるターゲット画像\n",
        "\n",
        "        # ランダムなindexの画像を取得 \n",
        "        AB_path = self.AB_paths[index]\n",
        "        AB = Image.open(AB_path).convert('RGB')\n",
        "\n",
        "        # 画像を2分割してAとBをそれぞれ取得\n",
        "        # ランダムシードの生成\n",
        "        param = self.__transform_param()\n",
        "        w, h = AB.size\n",
        "        w2 = int(w / 2)\n",
        "        # 256x256サイズの画像生成\n",
        "        # 一度リサイズしてランダムな位置で256x256にcropする\n",
        "        # AとBは同じ位置からcropする\n",
        "        transform = self.__transform(param)\n",
        "        A = transform(AB.crop((0, 0, w2, h)))\n",
        "        B = transform(AB.crop((w2, 0, w, h)))\n",
        "\n",
        "        #return {'A': A, 'B': B, 'A_paths': AB_path, 'B_paths': AB_path}\n",
        "        return {'A': B, 'B': A, 'A_paths': AB_path, 'B_paths': AB_path}\n",
        "\n",
        "    def __len__(self):\n",
        "        # 全画像ファイル数を返す\n",
        "        return len(self.AB_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnS2647IXDYL"
      },
      "source": [
        "## 生成器Gの処理定義\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Hg340TdPHA"
      },
      "source": [
        "# 生成器Gのクラス定義\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        # U-NetのEocoder部分\n",
        "        self.down0 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.down1 = self.__encoder_block(64, 128)\n",
        "        self.down2 = self.__encoder_block(128, 256)\n",
        "        self.down3 = self.__encoder_block(256, 512)\n",
        "        self.down4 = self.__encoder_block(512, 512)\n",
        "        self.down5 = self.__encoder_block(512, 512)\n",
        "        self.down6 = self.__encoder_block(512, 512)\n",
        "        self.down7 = self.__encoder_block(512, 512, use_norm=False)\n",
        "        # U-NetのDecoder部分\n",
        "        self.up7 = self.__decoder_block(512, 512)\n",
        "        self.up6 = self.__decoder_block(1024, 512, use_dropout=True)\n",
        "        self.up5 = self.__decoder_block(1024, 512, use_dropout=True)\n",
        "        self.up4 = self.__decoder_block(1024, 512, use_dropout=True)\n",
        "        self.up3 = self.__decoder_block(1024, 256)\n",
        "        self.up2 = self.__decoder_block(512, 128)\n",
        "        self.up1 = self.__decoder_block(256, 64)\n",
        "        # Gの最終出力\n",
        "        self.up0 = nn.Sequential(\n",
        "            self.__decoder_block(128, 3, use_norm=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def __encoder_block(self, input, output, use_norm=True):\n",
        "        # LeakyReLU＋Downsampling\n",
        "        layer = [\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(input, output, kernel_size=4, stride=2, padding=1)\n",
        "        ]\n",
        "        # BatchNormalization\n",
        "        if use_norm:\n",
        "            layer.append(nn.BatchNorm2d(output))\n",
        "        return nn.Sequential(*layer)\n",
        "\n",
        "    def __decoder_block(self, input, output, use_norm=True, use_dropout=False):\n",
        "        # ReLU＋Upsampling\n",
        "        layer = [\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(input, output, kernel_size=4, stride=2, padding=1)\n",
        "        ]\n",
        "        # BachNormalization\n",
        "        if use_norm:\n",
        "            layer.append(nn.BatchNorm2d(output))\n",
        "        # Dropout\n",
        "        if use_dropout:\n",
        "            layer.append(nn.Dropout(0.5))\n",
        "        return nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 偽物画像の生成\n",
        "        x0 = self.down0(x)\n",
        "        x1 = self.down1(x0)\n",
        "        x2 = self.down2(x1)\n",
        "        x3 = self.down3(x2)\n",
        "        x4 = self.down4(x3)\n",
        "        x5 = self.down5(x4)\n",
        "        x6 = self.down6(x5)\n",
        "        x7 = self.down7(x6)\n",
        "        y7 = self.up7(x7)\n",
        "        # Encoderの出力をDecoderの入力にSkipConnectionで接続\n",
        "        y6 = self.up6(self.concat(x6, y7))\n",
        "        y5 = self.up5(self.concat(x5, y6))\n",
        "        y4 = self.up4(self.concat(x4, y5))\n",
        "        y3 = self.up3(self.concat(x3, y4))\n",
        "        y2 = self.up2(self.concat(x2, y3))\n",
        "        y1 = self.up1(self.concat(x1, y2))\n",
        "        y0 = self.up0(self.concat(x0, y1))\n",
        "\n",
        "        return y0\n",
        "\n",
        "    def concat(self, x, y):\n",
        "        # 特徴マップの結合\n",
        "        return torch.cat([x, y], dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVcZnvSxXHle"
      },
      "source": [
        "## 識別器Dの処理定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBEO4lvYAq_K"
      },
      "source": [
        "# 識別器Dのクラス定義\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # 70x70PatchGAN識別器モデルの定義\n",
        "        # 2つの画像を結合したものが入力となるため、チャネル数は3*2=6となる\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            self.__layer(64, 128),\n",
        "            self.__layer(128, 256),\n",
        "            self.__layer(256, 512, stride=1),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def __layer(self, input, output, stride=2):\n",
        "        # DownSampling\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(input, output, kernel_size=4, stride=stride, padding=1),\n",
        "            nn.BatchNorm2d(output),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca-AexpDXLjC"
      },
      "source": [
        "## 敵対的損失関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSkNIq10Au81"
      },
      "source": [
        "# GANのAdversarial損失の定義(Real/Fake識別)\n",
        "class GANLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GANLoss, self).__init__()\n",
        "\n",
        "        self.register_buffer('real_label', torch.tensor(1.0))\n",
        "        self.register_buffer('fake_label', torch.tensor(0.0))\n",
        "        # Real/Fake識別の損失を、シグモイド＋バイナリクロスエントロピーで計算\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def __call__(self, prediction, is_real):\n",
        "        if is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "\n",
        "        return self.loss(prediction, target_tensor.expand_as(prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtKFmaK5XRdU"
      },
      "source": [
        "## Pix2Pixクラスの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-AswrHaAzKq"
      },
      "source": [
        "# Pix2Pixモデルの定義クラス\n",
        "# 入力と出力の画像ペア間のマッピングを学習するモデル\n",
        "class Pix2Pix():\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        # 生成器Gのオブジェクト取得とデバイス設定\n",
        "        self.netG = Generator().to(self.config.device)\n",
        "        # ネットワークの初期化\n",
        "        self.netG.apply(self.__weights_init)\n",
        "        # 生成器Gのモデルファイル読み込み(学習を引き続き行う場合)\n",
        "        if self.config.path_to_generator != None:\n",
        "            self.netG.load_state_dict(torch.load(self.config.path_to_generator, map_location=self.config.device_name), strict=False)\n",
        "\n",
        "        # 識別器Dのオブジェクト取得とデバイス設定\n",
        "        self.netD = Discriminator().to(self.config.device)\n",
        "        # Dのネットワーク初期化\n",
        "        self.netD.apply(self.__weights_init)\n",
        "        # Dのモデルファイル読み込み(学習を引き続き行う場合)\n",
        "        if self.config.path_to_discriminator != None:\n",
        "            self.netD.load_state_dict(torch.load(self.config.path_to_discriminator, map_location=self.config.device_name), strict=False)\n",
        "\n",
        "        # Optimizerの初期化\n",
        "        self.optimizerG = optim.Adam(self.netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        self.optimizerD = optim.Adam(self.netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        # 目的（損失関数)の設定\n",
        "        # GAN損失(Adversarial損失)\n",
        "        self.criterionGAN = GANLoss().to(self.config.device)\n",
        "        # L1損失\n",
        "        self.criterionL1 = nn.L1Loss()\n",
        "\n",
        "        # 学習率のスケジューラ設定\n",
        "        self.schedulerG = optim.lr_scheduler.LambdaLR(self.optimizerG, self.__modify_learning_rate)\n",
        "        self.schedulerD = optim.lr_scheduler.LambdaLR(self.optimizerD, self.__modify_learning_rate)\n",
        "\n",
        "        self.training_start_time = time.time()\n",
        "\n",
        "        self.writer = SummaryWriter(log_dir=config.log_dir)\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        # 学習率の更新、毎エポック後に呼ばれる\n",
        "        self.schedulerG.step()\n",
        "        self.schedulerD.step()\n",
        "\n",
        "    def __modify_learning_rate(self, epoch):\n",
        "        # 学習率の計算\n",
        "        # 指定の開始epochから、指定の減衰率で線形に減衰させる\n",
        "        if self.config.epochs_lr_decay_start < 0:\n",
        "            return 1.0\n",
        "\n",
        "        delta = max(0, epoch - self.config.epochs_lr_decay_start) / float(self.config.epochs_lr_decay)\n",
        "        return max(0.0, 1.0 - delta)\n",
        "\n",
        "    def __weights_init(self, m):\n",
        "        # パラメータ初期値の設定\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(1.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "    def train(self, data, batches_done):\n",
        "        # ドメインAのラベル画像とドメインBの正解画像を設定\n",
        "        self.realA = data['A'].to(self.config.device)\n",
        "        self.realB = data['B'].to(self.config.device)\n",
        "\n",
        "        # 生成器Gで画像生成\n",
        "        fakeB = self.netG(self.realA)\n",
        "\n",
        "        # Discriminator\n",
        "        # 条件画像(A)と生成画像(B)を結合\n",
        "        fakeAB = torch.cat((self.realA, fakeB), dim=1)\n",
        "        # 識別器Dに生成画像を入力、このときGは更新しないのでdetachして勾配は計算しない\n",
        "        pred_fake = self.netD(fakeAB.detach())\n",
        "        # 偽物画像を入力したときの識別器DのGAN損失を算出\n",
        "        lossD_fake = self.criterionGAN(pred_fake, False)\n",
        "\n",
        "        # 条件画像(A)と正解画像(B)を結合\n",
        "        realAB = torch.cat((self.realA, self.realB), dim=1)\n",
        "        # 識別器Dに正解画像を入力\n",
        "        pred_real = self.netD(realAB)\n",
        "        # 正解画像を入力したときの識別器DのGAN損失を算出\n",
        "        lossD_real = self.criterionGAN(pred_real, True)\n",
        "\n",
        "        # 偽物画像と正解画像のGAN損失の合計に0.5を掛ける\n",
        "        lossD = (lossD_fake + lossD_real) * 0.5\n",
        "\n",
        "        # Dの勾配をゼロに設定\n",
        "        self.optimizerD.zero_grad()\n",
        "        # Dの逆伝搬を計算\n",
        "        lossD.backward()\n",
        "        # Dの重みを更新\n",
        "        self.optimizerD.step()\n",
        "\n",
        "        # Generator\n",
        "        # 評価フェーズなので勾配は計算しない\n",
        "        # 識別器Dに生成画像を入力\n",
        "        with torch.no_grad():\n",
        "            pred_fake = self.netD(fakeAB)\n",
        "\n",
        "        # 生成器GのGAN損失を算出\n",
        "        lossG_GAN = self.criterionGAN(pred_fake, True)\n",
        "        # 生成器GのL1損失を算出\n",
        "        lossG_L1 = self.criterionL1(fakeB, self.realB) * self.config.lambda_L1\n",
        "\n",
        "        # 生成器Gの損失を合計\n",
        "        lossG = lossG_GAN + lossG_L1\n",
        "\n",
        "        # Gの勾配をゼロに設定\n",
        "        self.optimizerG.zero_grad()\n",
        "        # Gの逆伝搬を計算\n",
        "        lossG.backward()\n",
        "        # Gの重みを更新\n",
        "        self.optimizerG.step()\n",
        "\n",
        "        # for log\n",
        "        self.fakeB = fakeB\n",
        "        self.lossG_GAN = lossG_GAN\n",
        "        self.lossG_L1 = lossG_L1\n",
        "        self.lossG = lossG\n",
        "        self.lossD_real = lossD_real\n",
        "        self.lossD_fake = lossD_fake\n",
        "        self.lossD = lossD\n",
        "\n",
        "        train_info = {\n",
        "            'epoch': epoch, \n",
        "            'batch_num': batch_num,  \n",
        "            'lossG_GAN': lossG_GAN.item(),\n",
        "            'lossG_L1': lossG_L1.item(),\n",
        "            'lossG': lossG.item(),\n",
        "            'lossD_real': lossD_real.item(), \n",
        "            'lossD_fake': lossD_fake.item(), \n",
        "            'lossD': lossD.item(), \n",
        "            }\n",
        "\n",
        "        self.save_loss(train_info, batches_done)\n",
        "\n",
        "    def save_model(self, epoch):\n",
        "        # モデルの保存\n",
        "        output_dir = self.config.output_dir\n",
        "        torch.save(self.netG.state_dict(), '{}/pix2pix_G_epoch_{}'.format(output_dir, epoch))\n",
        "        torch.save(self.netD.state_dict(), '{}/pix2pix_D_epoch_{}'.format(output_dir, epoch))\n",
        "\n",
        "    def save_image(self, epoch):\n",
        "        # 条件画像、生成画像、正解画像を並べて画像を保存\n",
        "        output_image = torch.cat([self.realA, self.fakeB, self.realB], dim=3)\n",
        "        vutils.save_image(output_image,\n",
        "                '{}/pix2pix_epoch_{}.png'.format(self.config.output_dir, epoch),\n",
        "                normalize=True)\n",
        "\n",
        "        self.writer.add_image('image_epoch{}'.format(epoch), self.fakeB[0], epoch)\n",
        "\n",
        "    def save_loss(self, train_info, batches_done):\n",
        "        \"\"\"\n",
        "        lossの保存\n",
        "        \"\"\"\n",
        "        for k, v in train_info.items():\n",
        "            self.writer.add_scalar(k, v, batches_done)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_OfqQLYaHrA"
      },
      "source": [
        "# 学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5YmwuRnaQ_y"
      },
      "source": [
        "# 出力データ格納用フォルダ\n",
        "!mkdir -p 'output'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMXTvpOmj182"
      },
      "source": [
        "# パラメータの保存\n",
        "import json\n",
        "def save_json(file, save_path, mode):\n",
        "    with open(param_save_path, mode) as outfile:\n",
        "        json.dump(file, outfile, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnLViqRAQxtW"
      },
      "source": [
        "## TensorboardでのLossの確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNTNEifnm4zC"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "log_dir = './logs'\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir=log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu3JNcQnxNtN"
      },
      "source": [
        "!ls ./logs\n",
        "!rm -rf ./logs/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDp6tl7Qm6J8"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir './logs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXsjeaFjaFIp"
      },
      "source": [
        "## パラメータ設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGXFYaMPa-3r"
      },
      "source": [
        "\n",
        "class Opts():\n",
        "    def __init__(self):\n",
        "        self.epochs = 600\n",
        "        self.save_data_interval = 10\n",
        "        self.save_image_interval = 10\n",
        "        self.log_interval = 20\n",
        "        self.sample_interval = 10\n",
        "        self.batch_size = 64\n",
        "        self.load_size = 286\n",
        "        self.crop_size = 256\n",
        "        self.cpu = True\n",
        "        self.dataroot = 'datasets/facades'\n",
        "        self.output_dir = 'output'\n",
        "        self.log_dir = './logs'\n",
        "        self.phase = 'train'\n",
        "        self.lambda_L1 = 100.0\n",
        "        self.epochs_lr_decay = 0\n",
        "        self.epochs_lr_decay_start = -1\n",
        "        self.path_to_generator = None\n",
        "        self.path_to_discriminator = None\n",
        "        self.device_name = \"cuda:0\"\n",
        "        self.device = torch.device(self.device_name)\n",
        "    \n",
        "    def to_dict(self):\n",
        "        parameters = {\n",
        "            'epochs': self.epochs,\n",
        "            'save_data_interval': self.save_data_interval,\n",
        "            'save_image_interval': self.save_image_interval,\n",
        "            'log_interval': self.log_interval,\n",
        "            'sample_interval': self.sample_interval,\n",
        "            'batch_size': self.batch_size,\n",
        "            'load_size': self.load_size,\n",
        "            'crop_size': self.crop_size,\n",
        "            'cpu': self.cpu,\n",
        "            'dataroot': self.dataroot,\n",
        "            'output_dir': self.output_dir,\n",
        "            'log_dir': self.log_dir,\n",
        "            'phase': self.phase,\n",
        "            'lambda_L1': self.lambda_L1,\n",
        "            'epochs_lr_decay': self.epochs_lr_decay,\n",
        "            'epochs_lr_decay_start': self.epochs_lr_decay_start,\n",
        "            'path_to_generator': self.path_to_generator,\n",
        "            'path_to_discriminator': self.path_to_discriminator,\n",
        "            'device_name': self.device_name,\n",
        "        }\n",
        "        return parameters\n",
        "\n",
        "opt = Opts()\n",
        "\n",
        "param_save_path = os.path.join('output', 'param.json')\n",
        "save_json(opt.to_dict(), param_save_path, 'w')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJx58Vcxik0N"
      },
      "source": [
        "model = Pix2Pix(opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVaViivDmZ7D"
      },
      "source": [
        "dataset = AlignedDataset(opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmSvMfsydfZ3"
      },
      "source": [
        "dataloader = DataLoader(dataset, batch_size=opt.batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lotItvXCQoGi"
      },
      "source": [
        "## 学習の開始"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj3l33AFW3bt"
      },
      "source": [
        "for epoch in range(1, opt.epochs + 1):\n",
        "    for batch_num, data in enumerate(dataloader):\n",
        "        batches_done = (epoch - 1) * len(dataloader) + batch_num\n",
        "        model.train(data, batches_done)\n",
        "        \n",
        "        if batch_num % opt.log_interval == 0:\n",
        "            print(\"===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\".format(\n",
        "                epoch, batch_num, len(dataloader), model.lossD_real, model.lossG_GAN))\n",
        "\n",
        "    if epoch % opt.save_data_interval == 0:\n",
        "        model.save_model(epoch)\n",
        "\n",
        "    if epoch % opt.save_image_interval == 0:\n",
        "        model.save_image(epoch)\n",
        "\n",
        "    model.update_learning_rate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qchkd-YAQq-3"
      },
      "source": [
        "## 生成画像の確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y14gomepO4tc"
      },
      "source": [
        "# 生成画像の確認 (入力画像、生成画像、正解画像)\n",
        "from IPython.display import Image,display_png\n",
        "\n",
        "image_name = '{}/pix2pix_epoch_{}.png'.format('./output', opt.epochs)\n",
        "display_png(Image(image_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5uBLuNFPUZW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}